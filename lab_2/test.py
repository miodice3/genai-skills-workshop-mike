# -*- coding: utf-8 -*-
"""student-01-d0ec89363b76 (Dec 2, 2025, 1:54:00â€¯PM)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/qwiklabs-gcp-01-c75658565206/locations/us-central1/repositories/1e377bd1-dd18-415e-a25f-d3c8aa7a784e

google-cloud-bigquery (Client Library): This is the primary library for interacting with BigQuery from Python. It lets you create clients, run SQL queries, manage datasets/tables, and load data. This is what you'll use to execute the SQL commands from your outline.

google-genai (GenAI SDK): This SDK is used for direct interaction with the Gemini API for tasks like text generation. However, for the RAG flow you described, where the vector search and initial LLM calls are done within BigQuery ML (BQML), you will primarily use the google-cloud-bigquery library to run the BQML SQL functions. The final chatbot component could use google-genai for the conversational part, but keeping the entire RAG pipeline in BQML via the BigQuery client is often more efficient.

pandas: Essential for data manipulation and viewing query results returned from BigQuery.

google-cloud-bigquery-storage: Recommended dependency for faster data downloads.
"""

!pip install google-cloud-bigquery pandas google-cloud-bigquery-storage

import os
from google.cloud import bigquery
from google.cloud.bigquery import LoadJobConfig, SourceFormat
from IPython.display import display

# --- Configuration ---
PROJECT_ID = "qwiklabs-gcp-01-c75658565206"  # <-- **Replace this**
DATASET_ID = "aurora_rag_data"
TABLE_ID = "aurora_bay_faqs"
GCS_URI = "gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv"
LOCATION = "US"  # BigQuery dataset location (e.g., 'US' or 'EU')

# taken from UI
# Service account id: bqcx-720196750972-tcql@gcp-sa-bigquery-condel.iam.gserviceaccount.com
CONNECTION_ID = "embedding_conn" # manually created in bigquery UI

# manually grant IAM permissions for embedding_conn

# Initialize the BigQuery Client
bq_client = bigquery.Client(project=PROJECT_ID)

print(f"Project: {PROJECT_ID}")
print(f"Dataset: {DATASET_ID}")

# add IAM permissions manually via UI

dataset_ref = bq_client.dataset(DATASET_ID, project=PROJECT_ID)
dataset = bigquery.Dataset(dataset_ref)
dataset.location = LOCATION

try:
    dataset = bq_client.create_dataset(dataset, timeout=30)
    print(f"Created dataset {PROJECT_ID}.{DATASET_ID}")
except Exception as e:
    # Dataset likely already exists, which is fine
    if 'Already Exists' in str(e):
        print(f"Dataset {PROJECT_ID}.{DATASET_ID} already exists.")
    else:
        raise

# Define the schema based on the file content
schema = [
    bigquery.SchemaField("question", "STRING"),
    bigquery.SchemaField("answer", "STRING"),
]

job_config = LoadJobConfig(
    source_format=SourceFormat.CSV,
    skip_leading_rows=1,  # Assuming a header row
    autodetect=True,     # You can use autodetect for simplicity, or define the full schema
    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE, # Overwrite table if it exists
)

table_id_full = f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}"
load_job = bq_client.load_table_from_uri(
    GCS_URI,
    table_id_full,
    job_config=job_config,
)

print(f"Starting load job {load_job.job_id}...")
load_job.result() # Wait for the job to complete

print(f"Loaded {load_job.output_rows} rows into {table_id_full}")

# test validation of load job
# Display the first few rows to verify
query = f"SELECT * FROM `{table_id_full}` LIMIT 5"
df_source = bq_client.query(query).to_dataframe()
print("\n--- Source Table Preview ---")
display(df_source)

# create embedding and general query models

# Embedding
EMBEDDING_MODEL_NAME = "Embeddings"
EMBEDDING_MODEL_ID = "text-embedding-005"

embedding_model_sql = f"""
CREATE OR REPLACE MODEL `{DATASET_ID}.{EMBEDDING_MODEL_NAME}`
REMOTE WITH CONNECTION `{LOCATION}.{CONNECTION_ID}`
OPTIONS (ENDPOINT = '{EMBEDDING_MODEL_ID}');
"""
bq_client.query(embedding_model_sql).result()
print(f"Remote Embedding Model `{DATASET_ID}.{EMBEDDING_MODEL_NAME}` created/updated.")

# Query model
GEMINI_MODEL_NAME = "Gemini"
GEMINI_MODEL_ID = "gemini-2.5-flash"

gemini_model_sql = f"""
CREATE OR REPLACE MODEL `{DATASET_ID}.{GEMINI_MODEL_NAME}`
REMOTE WITH CONNECTION `{LOCATION}.{CONNECTION_ID}`
OPTIONS (ENDPOINT = '{GEMINI_MODEL_ID}');
"""
bq_client.query(gemini_model_sql).result()
print(f"Remote Gemini Model `{DATASET_ID}.{GEMINI_MODEL_NAME}` created/updated.")

# Check the actual column names and the first row of data
QUESTION_COL = "string_field_0"
ANSWER_COL = "string_field_1"

EMBEDDED_TABLE_ID = "aurora_bay_faqs_embedded"

embedding_query = f"""
CREATE OR REPLACE TABLE `{DATASET_ID}.{EMBEDDED_TABLE_ID}` AS
SELECT
    *
FROM
    ML.GENERATE_EMBEDDING(
        MODEL `{DATASET_ID}.{EMBEDDING_MODEL_NAME}`,
        (
            -- Use the actual column names here
            SELECT
                {QUESTION_COL} AS question, -- Alias back to 'question' for clarity in the final table
                {ANSWER_COL} AS answer,   -- Alias back to 'answer' for clarity in the final table
                CONCAT({QUESTION_COL}, ' ', {ANSWER_COL}) AS content -- The text used for embedding
            FROM
                `{DATASET_ID}.{TABLE_ID}`
        )
    )
"""

print("Generating embeddings and creating embedded table...")
# Execute the query
bq_client.query(embedding_query).result()
print(f"Table `{DATASET_ID}.{EMBEDDED_TABLE_ID}` created with embeddings.")

# Preview the embedded table
preview_query = f"SELECT question, answer, ml_generate_embedding_result FROM `{DATASET_ID}.{EMBEDDED_TABLE_ID}` LIMIT 3"
df_embedded = bq_client.query(preview_query).to_dataframe()
print("\n--- Embedded Table Preview ---")
# The ml_generate_embedding_result is a large array; show the first 5 elements for brevity
df_embedded['embedding_snippet'] = df_embedded['ml_generate_embedding_result'].apply(lambda x: str(x[:5]) + '...')
display(df_embedded[['question', 'embedding_snippet']])

def retrieve_context(user_question: str, top_k: int = 3) -> str:
    """
    Performs a vector search in BigQuery to find the top_k most relevant FAQ entries
    and joins the result back to the original embedded table to retrieve the question and answer.
    """

    # Escape single quotes in the user question for safe SQL embedding
    safe_user_question = user_question.replace("'", "''")

    search_query = f"""
    WITH QueryEmbedding AS (
        SELECT
            ml_generate_embedding_result AS query_vector
        FROM
            ML.GENERATE_EMBEDDING(
                MODEL `{DATASET_ID}.{EMBEDDING_MODEL_NAME}`,
                (SELECT '{safe_user_question}' AS content)
            )
    )

    -- 1. Perform the vector search and get the base_table_row_id
    , SearchResults AS (
        SELECT
            base_table_row_id,
            distance
        FROM
            VECTOR_SEARCH(
                TABLE `{DATASET_ID}.{EMBEDDED_TABLE_ID}`,
                'ml_generate_embedding_result',
                (SELECT query_vector FROM QueryEmbedding),
                top_k => {top_k}
            )
    )

    -- 2. Join the search results back to the embedded table using the row ID
    SELECT
        t2.question,
        t2.answer,
        t1.distance
    FROM
        SearchResults AS t1
    JOIN
        `{DATASET_ID}.{EMBEDDED_TABLE_ID}` AS t2
    ON
        t1.base_table_row_id = t2.base_table_row_id
    ORDER BY
        t1.distance ASC
    """

    df_results = bq_client.query(search_query).to_dataframe()

    context_list = [
        f"FAQ Question: {row['question']}\nFAQ Answer: {row['answer']}"
        for index, row in df_results.iterrows()
    ]

    context_string = "\n---\n".join(context_list)

    print(f"Retrieved {len(df_results)} relevant documents.")
    return context_string

def generate_response(user_question: str, context: str) -> str:
    """
    Uses the retrieved context and user question to generate a final answer with Gemini via BQML.
    """
    system_instruction = (
        "You are an expert chatbot for Aurora Bay. "
        "Use ONLY the provided context to answer the user's question. "
        "If the answer is not in the context, state that you cannot find the answer."
    )

    # Construct the final prompt with context
    prompt = (
        f"{system_instruction}\n\n"
        f"--- CONTEXT ---\n{context}\n\n"
        f"--- USER QUESTION ---\n{user_question}"
    )

    # We use a temporary table to pass the single prompt to ML.GENERATE_TEXT
    temp_table_id = f"{DATASET_ID}.temp_prompt"

    # 1. Create a temporary table with the prompt
    temp_table_query = f"""
    CREATE OR REPLACE TABLE `{temp_table_id}` AS
    SELECT '{prompt.replace("'", "''")}' AS prompt_text;
    """
    bq_client.query(temp_table_query).result()

    # 2. Call ML.GENERATE_TEXT with the prompt
    generation_query = f"""
    SELECT
        ml_generate_text_llm_result AS generated_response
    FROM
        ML.GENERATE_TEXT(
            MODEL `{DATASET_ID}.{GEMINI_MODEL_NAME}`,
            (SELECT prompt_text FROM `{temp_table_id}`),
            STRUCT(
                0.2 AS temperature,
                1024 AS max_output_tokens,
                TRUE AS flatten_json_output
            )
        );
    """

    df_response = bq_client.query(generation_query).to_dataframe()

    # 3. Clean up the temporary table
    bq_client.query(f"DROP TABLE IF EXISTS `{temp_table_id}`").result()

    return df_response['generated_response'].iloc[0]

user_query = "What are the rules for reserving a pool cabana and what is the maximum number of people?"
print(f"ðŸ‘¤ User Question: {user_query}")

# 1. Retrieve Context
context = retrieve_context(user_query, top_k=3)


# def rag_chatbot(question: str):
#     print(f"ðŸ‘¤ User Question: {question}")

#     # 1. Retrieve Context
#     context = retrieve_context(question, top_k=3)

#     # 2. Generate Response
#     response = generate_response(question, context)

#     print("\n-------------------------------------------")
#     print("ðŸ¤– Aurora Bay Chatbot Response:")
#     print(response)
#     print("-------------------------------------------")

# # --- Example Usage ---
# user_query = "What are the rules for reserving a pool cabana and what is the maximum number of people?"

# rag_chatbot(user_query)